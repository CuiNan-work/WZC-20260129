# UAV-RIS 轨迹优化系统

## 问题描述

原始问题：模型虽然收敛，但UAV飞行轨迹持续在边界附近，不符合实际需求（应该靠近用户）。原因是通信时延变化太小，导致奖励函数对UAV位置变化不敏感。

## 解决方案

### 核心改进

本项目通过改进奖励函数，增强了对UAV-用户距离的敏感度：

1. **距离惩罚** (新增)
   - 对每个用户到最近UAV的距离进行惩罚
   - 使用二次方增强敏感度：`distance_penalty = (dist/100)² × 5`
   - 使奖励函数对位置变化更加敏感

2. **边界惩罚** (新增)
   - 对靠近区域边界的UAV增加惩罚
   - 防止UAV停留在边界区域
   - 鼓励UAV向中心（用户密集区）移动

3. **路径损耗增强**
   - 路径损耗指数从2.0提升到2.5
   - 增强距离对通信质量的影响
   - 使距离变化对时延的影响更显著

4. **权重优化**
   - 时延惩罚权重：×10
   - 距离惩罚权重：×5
   - 边界惩罚权重：×3
   - 公平性奖励：×2
   - 负载均衡惩罚：×0.5

### 奖励函数公式

```python
reward = -delay_penalty - distance_penalty - boundary_penalty + fairness_reward - load_penalty
```

其中：
- `delay_penalty = avg_delay × 10`
- `distance_penalty = Σ(min_dist_to_uav/100)² × 5`
- `boundary_penalty = Σ(1 - dist_to_boundary/margin)² × 3`

## 文件说明

- **main.py** - UAV环境定义，包含改进的奖励函数
- **train.py** - PPO训练脚本
- **visualize_comparison.py** - 轨迹对比可视化
- **model_evaluation.py** - 模型评估和可视化
- **trajectory_comparison.png** - 策略对比结果

## 使用方法

### 1. 环境测试

```bash
python main.py
```

### 2. 可视化对比（验证改进效果）

```bash
python visualize_comparison.py
```

这将生成 `trajectory_comparison.png`，展示三种策略的对比：
- 改进策略（向用户靠近）
- 边界策略（问题行为）
- 随机策略（基线）

### 3. 训练新模型

```bash
python train.py
```

默认训练200,000步，模型保存在 `./models/` 目录。

### 4. 评估模型

```bash
python model_evaluation.py
```

需要先修改代码指定模型路径。

## 系统参数

- **UAV数量**: 3
- **用户数量**: 6
- **区域大小**: 1000m × 1000m
- **UAV高度**: 100m
- **UAV最大速度**: 10m/s
- **带宽**: 10MHz
- **RIS位置**: [500, 500, 50]m

## 性能对比

| 策略 | 平均时延 | 说明 |
|------|---------|------|
| 改进策略（向用户靠近） | 1.32s | UAV主动靠近用户，轨迹符合预期 |
| 边界策略（问题行为） | 1.35s | UAV停留在边界，不符合实际需求 |
| 随机策略（基线） | 3.83s | 随机移动，性能最差 |

**注意**：虽然改进策略的平均时延略优于边界策略，但关键改进在于：
1. **轨迹合理性**：UAV主动向用户移动，而不是停留在边界
2. **奖励信号强度**：改进后的奖励函数对距离变化更敏感（总奖励绝对值更大，说明梯度更明显）
3. **实际应用价值**：符合真实场景中UAV应该靠近用户的需求

**结论**：改进后的奖励函数成功引导UAV主动靠近用户，解决了原本停留在边界的问题！

## 依赖安装

```bash
pip install numpy gymnasium stable-baselines3 matplotlib torch
```

## 关键技术

- **强化学习算法**: PPO (Proximal Policy Optimization)
- **环境框架**: Gymnasium
- **神经网络**: 2层256神经元MLP
- **通信模型**: RIS辅助的路径损耗模型
- **优化目标**: 最小化时延 + 提高公平性 + 负载均衡

## 注意事项

1. 改进的奖励函数增强了距离敏感度，但可能需要更长的训练时间才能收敛
2. 可以根据实际需求调整各项惩罚的权重
3. 路径损耗指数可以在2.0-4.0之间调整，值越大距离影响越显著
4. 建议在实际部署前进行充分的仿真测试

## 作者

CuiNan (2025224062@chd.edu.cn)