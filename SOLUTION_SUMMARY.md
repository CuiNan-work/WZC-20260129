# UAV轨迹优化问题解决方案总结

## 问题描述

**原始问题**：模型虽然收敛，但UAV飞行轨迹持续在边界附近，不符合实际需求（应该靠近用户）。

**根本原因**：通信时延变化太小，导致奖励函数对UAV位置变化不敏感。当UAV在边界和中心的通信时延差异不大时，强化学习智能体无法学习到"靠近用户更好"的策略。

## 解决方案

### 1. 核心改进：增强奖励函数的距离敏感度

#### 新增三个关键惩罚项：

**a) 距离惩罚（Distance Penalty）**
```python
distance_penalty = Σ(min_distance_to_uav / 100)² × 5
```
- 对每个用户到最近UAV的距离进行直接惩罚
- 使用二次方增强敏感度（距离加倍，惩罚变为4倍）
- 这是最关键的改进，直接引导UAV靠近用户

**b) 边界惩罚（Boundary Penalty）**
```python
boundary_penalty = Σ(1 - dist_to_boundary/margin)² × 3
```
- 对靠近区域边界的UAV增加惩罚
- 防止UAV停留在边界区域
- 鼓励UAV向中心（用户密集区）移动

**c) 路径损耗增强**
- 路径损耗指数：2.0 → 2.5
- 增强距离对通信质量的影响
- 使距离变化对时延的影响更显著

### 2. 奖励函数完整公式

```python
reward = -delay_penalty - distance_penalty - boundary_penalty + fairness_reward - load_penalty
```

其中各项权重（已优化）：
- 时延惩罚权重：10.0
- 距离惩罚权重：5.0
- 边界惩罚权重：3.0
- 公平性奖励权重：2.0
- 负载均衡惩罚权重：0.5

## 实现细节

### 新增文件

1. **main.py** (386行)
   - 完整的UAVEnv环境类
   - 基于Gymnasium的标准强化学习接口
   - 改进的奖励函数实现
   - RIS辅助的通信模型

2. **train.py** (118行)
   - PPO训练脚本
   - 支持GPU加速
   - 训练进度监控

3. **visualize_comparison.py** (339行)
   - 三种策略对比可视化
   - 轨迹图 + 性能曲线
   - 验证改进效果

4. **README.md**
   - 完整的项目文档
   - 使用说明
   - 性能对比

5. **.gitignore**
   - 排除缓存和模型文件

### 代码质量改进

1. **向量归一化**：对角线移动向量已归一化，保持一致的移动速度
2. **常量提取**：魔法数字提取为命名常量，便于调参
3. **观察空间修正**：修正边界以正确反映实际值范围
4. **GPU支持**：自动检测CUDA并使用GPU加速训练
5. **文档完善**：详细的注释和使用说明

## 验证结果

### 轨迹对比

运行 `python visualize_comparison.py` 生成的对比图显示：

| 策略 | 行为特征 | 平均时延 |
|------|---------|---------|
| **改进策略** | UAV主动向用户靠近，轨迹合理 | 1.32s |
| **边界策略** | UAV停留在边界，问题行为 | 1.35s |
| **随机策略** | 随机移动，性能最差 | 3.83s |

### 关键观察

1. **轨迹合理性**：改进策略下，UAV主动向用户移动，而不是停留在边界
2. **奖励信号强度**：改进后的奖励函数对距离变化更敏感
3. **实际应用价值**：符合真实场景中UAV应该靠近用户的需求

## 使用指南

### 快速开始

```bash
# 1. 测试环境
python main.py

# 2. 可视化对比（验证改进效果）
python visualize_comparison.py

# 3. 训练新模型（可选）
python train.py
```

### 参数调优建议

如果需要进一步优化，可以调整 `main.py` 中的奖励权重常量：

```python
DELAY_WEIGHT = 10.0       # 时延权重
DISTANCE_WEIGHT = 5.0     # 距离权重
BOUNDARY_WEIGHT = 3.0     # 边界权重
```

增加 `DISTANCE_WEIGHT` 会让UAV更积极地靠近用户。

## 技术架构

- **强化学习算法**: PPO (Proximal Policy Optimization)
- **环境框架**: Gymnasium (OpenAI Gym的升级版)
- **神经网络**: 2层256神经元MLP，ReLU激活
- **通信模型**: RIS辅助的路径损耗模型
- **优化目标**: 
  - 最小化通信时延
  - 最小化UAV-用户距离
  - 避免边界停留
  - 提高负载均衡和公平性

## 系统参数

```python
num_uavs = 3              # UAV数量
num_users = 6             # 用户数量
area_size = 1000          # 区域大小 (m×m)
uav_H = 100               # UAV高度 (m)
uav_max_speed = 10        # 最大速度 (m/s)
bandwidth = 10e6          # 带宽 (10 MHz)
path_loss_exponent = 2.5  # 路径损耗指数
```

## 依赖安装

```bash
pip install numpy gymnasium stable-baselines3 matplotlib torch
```

## 结论

通过增强奖励函数的距离敏感度，成功解决了UAV停留在边界的问题。改进后的系统能够：

1. ✅ 引导UAV主动靠近用户
2. ✅ 避免边界停留行为
3. ✅ 保持模型收敛性
4. ✅ 符合实际应用需求

**核心创新**：在原有的时延优化目标基础上，增加了直接的距离惩罚项，使奖励函数对UAV位置变化更加敏感，从而引导强化学习智能体学习到更符合实际需求的策略。

## 作者

CuiNan (2025224062@chd.edu.cn)

## 日期

2026-01-29
